{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - pandas (YOUR NAME HERE)  \n",
    "### due 3/14/2022   \n",
    "\n",
    "Use this notebook and prompts to complete the homework. Throughout there will be hints and some code provided  \n",
    "\n",
    "### Things you will need:  \n",
    "- Install os, NumPy, pandas  \n",
    "- states_covid.csv  \n",
    "- Bloom_etal_2018_Reduced_Dataset.csv  \n",
    "- logfiles.tgz (or some other multiple file dataset)  \n",
    "\n",
    "*NOTE*: Make sure your PATH is correct  \n",
    "\n",
    "**import packages & check required datasets**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas' #CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'states_covid.csv')), 'states_covid.csv does not exist' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'Bloom_etal_2018_Reduced_Dataset.csv')), 'Bloom_etal_2018_Reduced_Dataset.csv does not exist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\"\n"
     ]
    }
   ],
   "source": [
    "#checking my current working directory\n",
    "!cd $PATH\n",
    "!echo \"$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas\n"
     ]
    }
   ],
   "source": [
    "cd $PATH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - DataFrame manipulation  \n",
    "\n",
    "Using **states_covid.csv**, we are going to read the data in as a DataFrame to manipulate, subset, and filter in various ways.   \n",
    "\n",
    "### Task 1.1 \n",
    "\n",
    "Read in states_covid.csv with date as a \"date\" dtype, and only columns consisting of the hospitalization (4 col), ICU (2 col), and Ventilators (2 col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'state', 'death', 'deathConfirmed', 'deathIncrease',\n",
       "       'deathProbable', 'hospitalized', 'hospitalizedCumulative',\n",
       "       'hospitalizedCurrently', 'hospitalizedIncrease', 'inIcuCumulative',\n",
       "       'inIcuCurrently', 'negative', 'negativeIncrease',\n",
       "       'negativeTestsAntibody', 'negativeTestsPeopleAntibody',\n",
       "       'negativeTestsViral', 'onVentilatorCumulative', 'onVentilatorCurrently',\n",
       "       'positive', 'positiveCasesViral', 'positiveIncrease', 'positiveScore',\n",
       "       'positiveTestsAntibody', 'positiveTestsAntigen',\n",
       "       'positiveTestsPeopleAntibody', 'positiveTestsPeopleAntigen',\n",
       "       'positiveTestsViral', 'recovered', 'totalTestEncountersViral',\n",
       "       'totalTestEncountersViralIncrease', 'totalTestResults',\n",
       "       'totalTestResultsIncrease', 'totalTestsAntibody', 'totalTestsAntigen',\n",
       "       'totalTestsPeopleAntibody', 'totalTestsPeopleAntigen',\n",
       "       'totalTestsPeopleViral', 'totalTestsPeopleViralIncrease',\n",
       "       'totalTestsViral', 'totalTestsViralIncrease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_df.columns #view the column names and find specified names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>hospitalizedCumulative</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>hospitalizedIncrease</th>\n",
       "      <th>inIcuCumulative</th>\n",
       "      <th>inIcuCurrently</th>\n",
       "      <th>onVentilatorCumulative</th>\n",
       "      <th>onVentilatorCurrently</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>122</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  hospitalized  hospitalizedCumulative  hospitalizedCurrently  \\\n",
       "0 2021-02-23        1260.0                  1260.0                   38.0   \n",
       "1 2021-02-23       45250.0                 45250.0                  762.0   \n",
       "2 2021-02-23       14617.0                 14617.0                  545.0   \n",
       "3 2021-02-23           NaN                     NaN                    NaN   \n",
       "4 2021-02-23       57072.0                 57072.0                 1515.0   \n",
       "\n",
       "   hospitalizedIncrease  inIcuCumulative  inIcuCurrently  \\\n",
       "0                     9              NaN             NaN   \n",
       "1                   122           2632.0             NaN   \n",
       "2                    47              NaN           204.0   \n",
       "3                     0              NaN             NaN   \n",
       "4                    78              NaN           447.0   \n",
       "\n",
       "   onVentilatorCumulative  onVentilatorCurrently  \n",
       "0                     NaN                    5.0  \n",
       "1                  1497.0                    NaN  \n",
       "2                  1505.0                   99.0  \n",
       "3                     NaN                    NaN  \n",
       "4                     NaN                  266.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_sub_df = pd.read_csv('states_covid.csv',usecols=['date','hospitalized', 'hospitalizedCumulative',\n",
    "       'hospitalizedCurrently', 'hospitalizedIncrease', 'inIcuCumulative','inIcuCurrently', 'onVentilatorCumulative', 'onVentilatorCurrently'],parse_dates=['date'],infer_datetime_format=True)\n",
    "state_covid_sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                      datetime64[ns]\n",
       "hospitalized                     float64\n",
       "hospitalizedCumulative           float64\n",
       "hospitalizedCurrently            float64\n",
       "hospitalizedIncrease               int64\n",
       "inIcuCumulative                  float64\n",
       "inIcuCurrently                   float64\n",
       "onVentilatorCumulative           float64\n",
       "onVentilatorCurrently            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_sub_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 \n",
    "\n",
    "For each of the following catergories: *currently* hospitalized, *currently* in the ICU, and *currently* on ventilation...  \n",
    "Find the 5 states with the greatest numbers in each catergory and list them in order.      \n",
    "*hint*: sort_values, unique  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (state_covid_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 states with the greatest number of currently hospitalized patients:\n",
      "state\n",
      "CA    22851.0\n",
      "NY    18825.0\n",
      "TX    14218.0\n",
      "FL     9520.0\n",
      "NJ     8270.0\n",
      "Name: hospitalizedCurrently, dtype: float64\n",
      "Top 5 states with the greatest number of currently in the ICU patients:\n",
      "state\n",
      "NY    5225.0\n",
      "CA    4971.0\n",
      "TX    3686.0\n",
      "NJ    2051.0\n",
      "MI    1663.0\n",
      "Name: inIcuCurrently, dtype: float64\n",
      "Top 5 states with the greatest number of currently on ventilation patients:\n",
      "state\n",
      "NY    2425.0\n",
      "NJ    1705.0\n",
      "MI    1441.0\n",
      "OH     863.0\n",
      "AZ     821.0\n",
      "Name: onVentilatorCurrently, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('states_covid.csv', parse_dates=['date'], usecols=['date', 'state', 'hospitalizedCurrently', 'hospitalizedCumulative', 'inIcuCurrently', 'inIcuCumulative', 'onVentilatorCurrently', 'onVentilatorCumulative'])\n",
    "\n",
    "# Find the 5 states with the greatest number of currently hospitalized patients\n",
    "currently_hospitalized = df.groupby('state')['hospitalizedCurrently'].max().sort_values(ascending=False)\n",
    "top_hospitalized = currently_hospitalized.head(5)\n",
    "print(\"Top 5 states with the greatest number of currently hospitalized patients:\")\n",
    "print(top_hospitalized)\n",
    "\n",
    "# Find the 5 states with the greatest number of currently in the ICU patients\n",
    "currently_icu = df.groupby('state')['inIcuCurrently'].max().sort_values(ascending=False)\n",
    "top_icu = currently_icu.head(5)\n",
    "print(\"Top 5 states with the greatest number of currently in the ICU patients:\")\n",
    "print(top_icu)\n",
    "\n",
    "# Find the 5 states with the greatest number of currently on ventilation patients\n",
    "currently_ventilator = df.groupby('state')['onVentilatorCurrently'].max().sort_values(ascending=False)\n",
    "top_ventilator = currently_ventilator.head(5)\n",
    "print(\"Top 5 states with the greatest number of currently on ventilation patients:\")\n",
    "print(top_ventilator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>hospitalizedCumulative</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>hospitalizedIncrease</th>\n",
       "      <th>inIcuCumulative</th>\n",
       "      <th>inIcuCurrently</th>\n",
       "      <th>onVentilatorCumulative</th>\n",
       "      <th>onVentilatorCurrently</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22851.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4811.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22836.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4905.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22820.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4731.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>2021-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22665.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22633.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4971.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  hospitalized  hospitalizedCumulative  hospitalizedCurrently  \\\n",
       "2637 2021-01-07           NaN                     NaN                22851.0   \n",
       "2581 2021-01-08           NaN                     NaN                22836.0   \n",
       "2693 2021-01-06           NaN                     NaN                22820.0   \n",
       "2357 2021-01-12           NaN                     NaN                22665.0   \n",
       "2413 2021-01-11           NaN                     NaN                22633.0   \n",
       "\n",
       "      hospitalizedIncrease  inIcuCumulative  inIcuCurrently  \\\n",
       "2637                     0              NaN          4811.0   \n",
       "2581                     0              NaN          4905.0   \n",
       "2693                     0              NaN          4731.0   \n",
       "2357                     0              NaN          4962.0   \n",
       "2413                     0              NaN          4971.0   \n",
       "\n",
       "      onVentilatorCumulative  onVentilatorCurrently  \n",
       "2637                     NaN                    NaN  \n",
       "2581                     NaN                    NaN  \n",
       "2693                     NaN                    NaN  \n",
       "2357                     NaN                    NaN  \n",
       "2413                     NaN                    NaN  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_state_covid_sub_df = new_state_covid_sub_df.sort_values('positive',ascending=False,inplace=False)\n",
    "new_state_covid_sub_df.sort_values('hospitalizedCurrently',ascending=False,inplace=True)\n",
    "new_state_covid_sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 \n",
    "\n",
    "Find the date in which each state crossed 1000 cumulative hospitilized covid patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK: 2021-02-23 00:00:00\n",
      "AL: 2021-02-23 00:00:00\n",
      "AR: 2021-02-23 00:00:00\n",
      "AS: 2021-02-23 00:00:00\n",
      "AZ: 2021-02-23 00:00:00\n",
      "CA: 2021-02-23 00:00:00\n",
      "CO: 2021-02-23 00:00:00\n",
      "CT: 2021-02-23 00:00:00\n",
      "DC: 2021-02-23 00:00:00\n",
      "DE: 2021-02-23 00:00:00\n",
      "FL: 2021-02-23 00:00:00\n",
      "GA: 2021-02-23 00:00:00\n",
      "GU: 2021-02-23 00:00:00\n",
      "HI: 2021-02-23 00:00:00\n",
      "IA: 2021-02-23 00:00:00\n",
      "ID: 2021-02-23 00:00:00\n",
      "IL: 2021-02-23 00:00:00\n",
      "IN: 2021-02-23 00:00:00\n",
      "KS: 2021-02-23 00:00:00\n",
      "KY: 2021-02-23 00:00:00\n",
      "LA: 2021-02-23 00:00:00\n",
      "MA: 2021-02-23 00:00:00\n",
      "MD: 2021-02-23 00:00:00\n",
      "ME: 2021-02-23 00:00:00\n",
      "MI: 2021-02-23 00:00:00\n",
      "MN: 2021-02-23 00:00:00\n",
      "MO: 2021-02-23 00:00:00\n",
      "MP: 2021-02-23 00:00:00\n",
      "MS: 2021-02-23 00:00:00\n",
      "MT: 2021-02-23 00:00:00\n",
      "NC: 2021-02-23 00:00:00\n",
      "ND: 2021-02-23 00:00:00\n",
      "NE: 2021-02-23 00:00:00\n",
      "NH: 2021-02-23 00:00:00\n",
      "NJ: 2021-02-23 00:00:00\n",
      "NM: 2021-02-23 00:00:00\n",
      "NV: 2021-02-23 00:00:00\n",
      "NY: 2021-02-23 00:00:00\n",
      "OH: 2021-02-23 00:00:00\n",
      "OK: 2021-02-23 00:00:00\n",
      "OR: 2021-02-23 00:00:00\n",
      "PA: 2020-04-06 00:00:00\n",
      "PR: 2021-02-23 00:00:00\n",
      "RI: 2021-02-23 00:00:00\n",
      "SC: 2021-02-23 00:00:00\n",
      "SD: 2021-02-23 00:00:00\n",
      "TN: 2021-02-23 00:00:00\n",
      "TX: 2021-02-23 00:00:00\n",
      "UT: 2021-02-23 00:00:00\n",
      "VA: 2021-02-23 00:00:00\n",
      "VI: 2021-02-23 00:00:00\n",
      "VT: 2021-02-23 00:00:00\n",
      "WA: 2021-02-23 00:00:00\n",
      "WI: 2021-02-23 00:00:00\n",
      "WV: 2021-02-23 00:00:00\n",
      "WY: 2021-02-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#group by states using a for loop \n",
    "\n",
    "# Create a new DataFrame with only the necessary columns and group by state\n",
    "df_hospitalized_cumulative = df[['state', 'hospitalizedCumulative', 'date']].groupby('state')\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Loop through each group and find the date of the first instance where the cumulative hospitalized count exceeds 1000\n",
    "for state, group in df_hospitalized_cumulative:\n",
    "    idx = (group['hospitalizedCumulative'] >= 1000).idxmax()  # find the index of the first instance where the cumulative count exceeds 1000\n",
    "    date = group.loc[idx, 'date']  # extract the date corresponding to the index\n",
    "    results[state] = date  # add the state and date to the results dictionary\n",
    "\n",
    "# Print the results\n",
    "for state, date in results.items():\n",
    "    print(f\"{state}: {date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - DataFrame summarizing  \n",
    "\n",
    "Using **Bloom_etal_2018_Reduced_Dataset.csv**, we are going to do more dataframe manipulation and subsetting and summarizing    \n",
    "\n",
    "### Task 2.1 \n",
    "\n",
    "Read in Bloom_etal_2018_Reduced_Dataset.csv and create two new columns ('genus','species') that consists of the column *taxa* split at the underscore. Print out the head of this new dataframe and the number of unique genera**     \n",
    "\n",
    "*hint:* pd.str.split(,expand=True)  \n",
    "\n",
    "for example:  \n",
    "\n",
    "| taxa | genus | species   \n",
    "| :------ | :-- | :---   \n",
    "| Alosa_alabamae | Alosa | alabamae  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   taxa  genus         species\n",
      "0        Alosa_alabamae  Alosa        alabamae\n",
      "1           Alosa_alosa  Alosa           alosa\n",
      "2          Alosa_fallax  Alosa          fallax\n",
      "3       Alosa_mediocris  Alosa       mediocris\n",
      "4  Alosa_pseudoharengus  Alosa  pseudoharengus\n",
      "Number of unique genera: 34\n"
     ]
    }
   ],
   "source": [
    "# Read in the CSV file\n",
    "df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv')\n",
    "\n",
    "# Split the 'taxa' column into two new columns: 'genus' and 'species'\n",
    "df[['genus', 'species']] = df['taxa'].str.split('_', n=1, expand=True)\n",
    "\n",
    "# Print out the head of the new dataframe with the taxa, genus, and species columns\n",
    "print(df[['taxa', 'genus', 'species']].head())\n",
    "\n",
    "# Print out the number of unique genera\n",
    "print(\"Number of unique genera:\", df['genus'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 \n",
    "\n",
    "Create a new dataframe with the mean *logbodysize* and *trophicposition* of each genera. Sort this data frame by the largest body size. Print the head of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['taxa', 'logbodysize', 'trophic_position', 'Reg'], dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the CSV file\n",
    "new_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv')\n",
    "\n",
    "#I was getting errors so I had to find the name of the columns since trophicposition in the data was underscored\n",
    "new_df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                logbodysize  trophic_position\n",
      "genus                                        \n",
      "Tenualosa          1.778151          0.462398\n",
      "Alosa              1.739062          0.540815\n",
      "Coilia             1.544068          0.477121\n",
      "Ethmalosa          1.544068          0.397940\n",
      "Potamalosa         1.505150          0.518514\n",
      "Hilsa              1.387390          0.462398\n",
      "Anodontostoma      1.342423          0.447158\n",
      "Engraulis          1.190332          0.462398\n",
      "Cetengraulis       1.113943          0.322219\n",
      "Pellona            1.113943          0.623249\n",
      "Anchovia           1.093422          0.477121\n",
      "Thryssa            1.041393          0.447158\n",
      "Pellonula          1.004300          0.504938\n",
      "Escualosa          1.000000          0.505150\n",
      "Gilchristella      1.000000          0.491362\n",
      "Stolothrissa       1.000000          0.431364\n",
      "Hyperlophus        1.000000          0.531479\n",
      "Stolephorus        0.973128          0.518514\n",
      "Chirocentrodon     0.954243          0.518514\n",
      "Encrasicholina     0.903090          0.518514\n",
      "Clupeoides         0.903090          0.491362\n",
      "Ramnogaster        0.903090          0.531479\n",
      "Rhinosardinia      0.903090          0.531479\n",
      "Spratelloides      0.845098          0.491362\n",
      "Anchoa             0.843681          0.522218\n",
      "Jenkinsia          0.838849          0.518514\n",
      "Microthrissa       0.778151          0.491362\n",
      "Anchoviella        0.775207          0.496877\n",
      "Ehirava            0.698970          0.491362\n",
      "Sauvagella         0.672098          0.491362\n",
      "Clupeichthys       0.602060          0.477121\n",
      "Sierrathrissa      0.477121          0.491362\n",
      "Sundasalanx        0.447158          0.491362\n",
      "Amazonsprattus     0.301030          0.531479\n"
     ]
    }
   ],
   "source": [
    "# Read in the CSV file\n",
    "new_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv')\n",
    "\n",
    "# Split the 'taxa' column into two new columns: 'genus' and 'species'\n",
    "new_df[['genus', 'species']] = new_df['taxa'].str.split('_', n=1, expand=True)\n",
    "\n",
    "# Group by genus and calculate the mean logbodysize and trophicposition\n",
    "genus_means = new_df.groupby('genus').agg({'logbodysize': 'mean', 'trophic_position': 'mean'})\n",
    "\n",
    "# Sort the dataframe by the mean logbodysize in descending order\n",
    "genus_means = genus_means.sort_values('logbodysize', ascending=False)\n",
    "\n",
    "# Print the head of the new dataframe\n",
    "print(genus_means.head(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which genera is the smallest and largest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest genus: Amazonsprattus\n",
      "Largest genus: Tenualosa\n"
     ]
    }
   ],
   "source": [
    "# Find the genus with the smallest mean logbodysize\n",
    "smallest_genus = genus_means['logbodysize'].idxmin()\n",
    "print(\"Smallest genus:\", smallest_genus)\n",
    "\n",
    "# Find the genus with the largest mean logbodysize\n",
    "largest_genus = genus_means['logbodysize'].idxmax()\n",
    "print(\"Largest genus:\", largest_genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the trophic position of the smallest and largest?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest genus: Amazonsprattus\n",
      "Smallest genus trophic position: 0.531478917\n",
      "Largest genus: Tenualosa\n",
      "Largest genus trophic position: 0.462397998\n"
     ]
    }
   ],
   "source": [
    "# Find the genus with the smallest mean logbodysize and its corresponding trophic position\n",
    "smallest_genus = genus_means['logbodysize'].idxmin()\n",
    "smallest_trophic_position = genus_means.loc[smallest_genus]['trophic_position']\n",
    "print(\"Smallest genus:\", smallest_genus)\n",
    "print(\"Smallest genus trophic position:\", smallest_trophic_position)\n",
    "\n",
    "# Find the genus with the largest mean logbodysize and its corresponding trophic position\n",
    "largest_genus = genus_means['logbodysize'].idxmax()\n",
    "largest_trophic_position = genus_means.loc[largest_genus]['trophic_position']\n",
    "print(\"Largest genus:\", largest_genus)\n",
    "print(\"Largest genus trophic position:\", largest_trophic_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Read in muliple files to a dictionary and make a DataFrame - **OPTIONAL/BONUS**  \n",
    "\n",
    "### This is not something you are expected to do in this course, but just here to give you an idea of the things that you COULD do. Answers will be posted after due date.  \n",
    "\n",
    "\n",
    "Using **logfiles**: we are going to do read in each file, get some data, append it to a dictionary to later make into a dataframe.     \n",
    "\n",
    "**note:** *make sure to unzip logfiles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\"\n"
     ]
    }
   ],
   "source": [
    "PATH = 'C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas'\n",
    "\n",
    "!cd $PATH\n",
    "!echo \"$PATH\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas\n",
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 0038-28BC\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas\n",
      "\n",
      "03/14/2023  11:34 PM    <DIR>          .\n",
      "03/14/2023  11:34 PM    <DIR>          ..\n",
      "02/15/2022  11:20 AM               210 ._1901302108_H7_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302109_H7_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302110_H13_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302115_H1_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302117_H15_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302118_H1_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302119_H4_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302120_H5_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302121_H8_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302122_H3_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302134_H13_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302136_H15_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302137_H18_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302138_H11_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302141_H19_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302146_H14_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302150_H14_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302157_H12_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302158_H9_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302194_H10_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302202_H16_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302203_H7_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302212_H9_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302214_H19_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302217_H18_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302223_H16_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302224_H3_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302225_H12_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302227_H11_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302228_H4_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302235_H6_S_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302236_H17_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302237_H10_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302240_H8_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302241_H2_D_14.txt.txt\n",
      "02/15/2022  11:20 AM               210 ._1901302243_H5_S_14.txt.txt\n",
      "02/15/2022  11:25 AM               210 ._logfiles\n",
      "03/14/2023  10:14 PM    <DIR>          .ipynb_checkpoints\n",
      "02/15/2022  11:20 AM           450,331 1901302108_H7_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,866 1901302109_H7_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,016 1901302110_H13_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,389 1901302115_H1_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           453,384 1901302117_H15_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,994 1901302118_H1_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,122 1901302119_H4_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,194 1901302120_H5_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           449,579 1901302121_H8_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           464,262 1901302122_H3_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,475 1901302134_H13_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,858 1901302136_H15_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,433 1901302137_H18_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           453,648 1901302138_H11_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           486,044 1901302141_H19_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           451,668 1901302146_H14_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           453,310 1901302150_H14_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,500 1901302157_H12_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,864 1901302158_H9_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           470,965 1901302194_H10_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,396 1901302202_H16_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           450,293 1901302203_H7_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           451,127 1901302212_H9_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           486,716 1901302214_H19_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,121 1901302217_H18_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           453,150 1901302223_H16_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           467,852 1901302224_H3_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           470,949 1901302225_H12_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           453,411 1901302227_H11_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,724 1901302228_H4_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           469,186 1901302235_H6_S_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,102 1901302236_H17_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           471,052 1901302237_H10_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           452,357 1901302240_H8_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           469,843 1901302241_H2_D_14.txt.txt\n",
      "02/15/2022  11:20 AM           468,606 1901302243_H5_S_14.txt.txt\n",
      "03/07/2023  04:12 PM             2,919 Bloom_etal_2018_Reduced_Dataset.csv\n",
      "03/14/2023  11:34 PM            57,373 Homework 4_Pandas_Jeshurun.ipynb\n",
      "03/13/2023  11:53 PM            61,766 Homework4-pandas.ipynb\n",
      "03/14/2023  12:03 AM    <DIR>          logfiles\n",
      "03/07/2023  04:12 PM        16,765,440 logfiles.tgz\n",
      "03/09/2023  04:44 PM            94,570 primer_python-pandas.ipynb\n",
      "03/07/2023  04:12 PM         6,354,563 pythonModules.pptx\n",
      "03/07/2023  04:12 PM         2,683,954 states_covid.csv\n",
      "03/07/2023  04:30 PM               589 Untitled.ipynb\n",
      "              81 File(s)     42,688,731 bytes\n",
      "               4 Dir(s)  110,094,471,168 bytes free\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\n",
    "%ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to specify all the path for it to read, I couldn't read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x logfiles/\n",
      "x logfiles/._1901302235_H6_S_14.txt.txt\n",
      "x logfiles/1901302235_H6_S_14.txt.txt\n",
      "x logfiles/._1901302121_H8_S_14.txt.txt\n",
      "x logfiles/1901302121_H8_S_14.txt.txt\n",
      "x logfiles/._1901302217_H18_D_14.txt.txt\n",
      "x logfiles/1901302217_H18_D_14.txt.txt\n",
      "x logfiles/._1901302110_H13_S_14.txt.txt\n",
      "x logfiles/1901302110_H13_S_14.txt.txt\n",
      "x logfiles/._1901302146_H14_D_14.txt.txt\n",
      "x logfiles/1901302146_H14_D_14.txt.txt\n",
      "x logfiles/._1901302240_H8_D_14.txt.txt\n",
      "x logfiles/1901302240_H8_D_14.txt.txt\n",
      "x logfiles/._1901302109_H7_S_14.txt.txt\n",
      "x logfiles/1901302109_H7_S_14.txt.txt\n",
      "x logfiles/._1901302150_H14_S_14.txt.txt\n",
      "x logfiles/1901302150_H14_S_14.txt.txt\n",
      "x logfiles/._1901302141_H19_S_14.txt.txt\n",
      "x logfiles/1901302141_H19_S_14.txt.txt\n",
      "x logfiles/._1901302241_H2_D_14.txt.txt\n",
      "x logfiles/1901302241_H2_D_14.txt.txt\n",
      "x logfiles/._1901302194_H10_S_14.txt.txt\n",
      "x logfiles/1901302194_H10_S_14.txt.txt\n",
      "x logfiles/._1901302227_H11_S_14.txt.txt\n",
      "x logfiles/1901302227_H11_S_14.txt.txt\n",
      "x logfiles/._1901302136_H15_D_14.txt.txt\n",
      "x logfiles/1901302136_H15_D_14.txt.txt\n",
      "x logfiles/._1901302223_H16_S_14.txt.txt\n",
      "x logfiles/1901302223_H16_S_14.txt.txt\n",
      "x logfiles/._1901302243_H5_S_14.txt.txt\n",
      "x logfiles/1901302243_H5_S_14.txt.txt\n",
      "x logfiles/._1901302122_H3_D_14.txt.txt\n",
      "x logfiles/1901302122_H3_D_14.txt.txt\n",
      "x logfiles/._1901302202_H16_S_14.txt.txt\n",
      "x logfiles/1901302202_H16_S_14.txt.txt\n",
      "x logfiles/._1901302236_H17_D_14.txt.txt\n",
      "x logfiles/1901302236_H17_D_14.txt.txt\n",
      "x logfiles/._1901302158_H9_S_14.txt.txt\n",
      "x logfiles/1901302158_H9_S_14.txt.txt\n",
      "x logfiles/._1901302225_H12_D_14.txt.txt\n",
      "x logfiles/1901302225_H12_D_14.txt.txt\n",
      "x logfiles/._1901302120_H5_D_14.txt.txt\n",
      "x logfiles/1901302120_H5_D_14.txt.txt\n",
      "x logfiles/._1901302118_H1_D_14.txt.txt\n",
      "x logfiles/1901302118_H1_D_14.txt.txt\n",
      "x logfiles/._1901302203_H7_S_14.txt.txt\n",
      "x logfiles/1901302203_H7_S_14.txt.txt\n",
      "x logfiles/._1901302119_H4_S_14.txt.txt\n",
      "x logfiles/1901302119_H4_S_14.txt.txt\n",
      "x logfiles/._1901302212_H9_D_14.txt.txt\n",
      "x logfiles/1901302212_H9_D_14.txt.txt\n",
      "x logfiles/._1901302108_H7_D_14.txt.txt\n",
      "x logfiles/1901302108_H7_D_14.txt.txt\n",
      "x logfiles/._1901302224_H3_S_14.txt.txt\n",
      "x logfiles/1901302224_H3_S_14.txt.txt\n",
      "x logfiles/._1901302237_H10_D_14.txt.txt\n",
      "x logfiles/1901302237_H10_D_14.txt.txt\n",
      "x logfiles/._1901302117_H15_S_14.txt.txt\n",
      "x logfiles/1901302117_H15_S_14.txt.txt\n",
      "x logfiles/._1901302137_H18_S_14.txt.txt\n",
      "x logfiles/1901302137_H18_S_14.txt.txt\n",
      "x logfiles/._1901302214_H19_D_14.txt.txt\n",
      "x logfiles/1901302214_H19_D_14.txt.txt\n",
      "x logfiles/._1901302134_H13_D_14.txt.txt\n",
      "x logfiles/1901302134_H13_D_14.txt.txt\n",
      "x logfiles/._1901302157_H12_S_14.txt.txt\n",
      "x logfiles/1901302157_H12_S_14.txt.txt\n",
      "x logfiles/._1901302138_H11_D_14.txt.txt\n",
      "x logfiles/1901302138_H11_D_14.txt.txt\n",
      "x logfiles/._1901302228_H4_D_14.txt.txt\n",
      "x logfiles/1901302228_H4_D_14.txt.txt\n",
      "x logfiles/._1901302115_H1_S_14.txt.txt\n",
      "x logfiles/1901302115_H1_S_14.txt.txt\n",
      "tar: #unzip: Not found in archive\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf \"C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas/logfiles.tgz\" #unzip logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(PATH,'logfiles')\n",
    "assert os.path.exists(log_dir), 'log_dir does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to find the necessary files. The number of files in the log files is 36, make sure you have that many as well  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "._1901302108_H7_D_14.txt.txt\n",
      "._1901302109_H7_S_14.txt.txt\n",
      "._1901302110_H13_S_14.txt.txt\n",
      "._1901302115_H1_S_14.txt.txt\n",
      "._1901302117_H15_S_14.txt.txt\n",
      "._1901302118_H1_D_14.txt.txt\n",
      "._1901302119_H4_S_14.txt.txt\n",
      "._1901302120_H5_D_14.txt.txt\n",
      "._1901302121_H8_S_14.txt.txt\n",
      "._1901302122_H3_D_14.txt.txt\n",
      "._1901302134_H13_D_14.txt.txt\n",
      "._1901302136_H15_D_14.txt.txt\n",
      "._1901302137_H18_S_14.txt.txt\n",
      "._1901302138_H11_D_14.txt.txt\n",
      "._1901302141_H19_S_14.txt.txt\n",
      "._1901302146_H14_D_14.txt.txt\n",
      "._1901302150_H14_S_14.txt.txt\n",
      "._1901302157_H12_S_14.txt.txt\n",
      "._1901302158_H9_S_14.txt.txt\n",
      "._1901302194_H10_S_14.txt.txt\n",
      "._1901302202_H16_S_14.txt.txt\n",
      "._1901302203_H7_S_14.txt.txt\n",
      "._1901302212_H9_D_14.txt.txt\n",
      "._1901302214_H19_D_14.txt.txt\n",
      "._1901302217_H18_D_14.txt.txt\n",
      "._1901302223_H16_S_14.txt.txt\n",
      "._1901302224_H3_S_14.txt.txt\n",
      "._1901302225_H12_D_14.txt.txt\n",
      "._1901302227_H11_S_14.txt.txt\n",
      "._1901302228_H4_D_14.txt.txt\n",
      "._1901302235_H6_S_14.txt.txt\n",
      "._1901302236_H17_D_14.txt.txt\n",
      "._1901302237_H10_D_14.txt.txt\n",
      "._1901302240_H8_D_14.txt.txt\n",
      "._1901302241_H2_D_14.txt.txt\n",
      "._1901302243_H5_S_14.txt.txt\n",
      "._logfiles\n",
      "1901302108_H7_D_14.txt.txt\n",
      "1901302109_H7_S_14.txt.txt\n",
      "1901302110_H13_S_14.txt.txt\n",
      "1901302115_H1_S_14.txt.txt\n",
      "1901302117_H15_S_14.txt.txt\n",
      "1901302118_H1_D_14.txt.txt\n",
      "1901302119_H4_S_14.txt.txt\n",
      "1901302120_H5_D_14.txt.txt\n",
      "1901302121_H8_S_14.txt.txt\n",
      "1901302122_H3_D_14.txt.txt\n",
      "1901302134_H13_D_14.txt.txt\n",
      "1901302136_H15_D_14.txt.txt\n",
      "1901302137_H18_S_14.txt.txt\n",
      "1901302138_H11_D_14.txt.txt\n",
      "1901302141_H19_S_14.txt.txt\n",
      "1901302146_H14_D_14.txt.txt\n",
      "1901302150_H14_S_14.txt.txt\n",
      "1901302157_H12_S_14.txt.txt\n",
      "1901302158_H9_S_14.txt.txt\n",
      "1901302194_H10_S_14.txt.txt\n",
      "1901302202_H16_S_14.txt.txt\n",
      "1901302203_H7_S_14.txt.txt\n",
      "1901302212_H9_D_14.txt.txt\n",
      "1901302214_H19_D_14.txt.txt\n",
      "1901302217_H18_D_14.txt.txt\n",
      "1901302223_H16_S_14.txt.txt\n",
      "1901302224_H3_S_14.txt.txt\n",
      "1901302225_H12_D_14.txt.txt\n",
      "1901302227_H11_S_14.txt.txt\n",
      "1901302228_H4_D_14.txt.txt\n",
      "1901302235_H6_S_14.txt.txt\n",
      "1901302236_H17_D_14.txt.txt\n",
      "1901302237_H10_D_14.txt.txt\n",
      "1901302240_H8_D_14.txt.txt\n",
      "1901302241_H2_D_14.txt.txt\n",
      "1901302243_H5_S_14.txt.txt\n",
      "Bloom_etal_2018_Reduced_Dataset.csv\n",
      "Homework 4_Pandas_Jeshurun.ipynb\n",
      "Homework4-pandas.ipynb\n",
      "logfiles\n",
      "logfiles.tgz\n",
      "primer_python-pandas.ipynb\n",
      "pythonModules.pptx\n",
      "states_covid.csv\n",
      "Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\".\")\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:/Users/Administrator/Desktop/BIOL_792/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901302108_H7_D_14.txt.txt\n",
      "1901302109_H7_S_14.txt.txt\n",
      "1901302110_H13_S_14.txt.txt\n",
      "1901302115_H1_S_14.txt.txt\n",
      "1901302117_H15_S_14.txt.txt\n",
      "1901302118_H1_D_14.txt.txt\n",
      "1901302119_H4_S_14.txt.txt\n",
      "1901302120_H5_D_14.txt.txt\n",
      "1901302121_H8_S_14.txt.txt\n",
      "1901302122_H3_D_14.txt.txt\n",
      "1901302134_H13_D_14.txt.txt\n",
      "1901302136_H15_D_14.txt.txt\n",
      "1901302137_H18_S_14.txt.txt\n",
      "1901302138_H11_D_14.txt.txt\n",
      "1901302141_H19_S_14.txt.txt\n",
      "1901302146_H14_D_14.txt.txt\n",
      "1901302150_H14_S_14.txt.txt\n",
      "1901302157_H12_S_14.txt.txt\n",
      "1901302158_H9_S_14.txt.txt\n",
      "1901302194_H10_S_14.txt.txt\n",
      "1901302202_H16_S_14.txt.txt\n",
      "1901302203_H7_S_14.txt.txt\n",
      "1901302212_H9_D_14.txt.txt\n",
      "1901302214_H19_D_14.txt.txt\n",
      "1901302217_H18_D_14.txt.txt\n",
      "1901302223_H16_S_14.txt.txt\n",
      "1901302224_H3_S_14.txt.txt\n",
      "1901302225_H12_D_14.txt.txt\n",
      "1901302227_H11_S_14.txt.txt\n",
      "1901302228_H4_D_14.txt.txt\n",
      "1901302235_H6_S_14.txt.txt\n",
      "1901302236_H17_D_14.txt.txt\n",
      "1901302237_H10_D_14.txt.txt\n",
      "1901302240_H8_D_14.txt.txt\n",
      "1901302241_H2_D_14.txt.txt\n",
      "1901302243_H5_S_14.txt.txt\n",
      "All log files accounted for.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the directory path to the folder containing the log files\n",
    "directory = r'C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas'\n",
    "\n",
    "# Use glob to get all the log file paths in the directory\n",
    "log_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "\n",
    "# Loop over the log files and print out their names\n",
    "for log_file in log_files:\n",
    "    print(os.path.basename(log_file))\n",
    "\n",
    "# Check that the number of log files is equal to 36\n",
    "if len(log_files) != 36:\n",
    "    print(f\"The number of log files ({len(log_files)}) is not equal to 36.\")\n",
    "    # Alternatively, you could raise an AssertionError to terminate the program:\n",
    "    # raise AssertionError(f\"The number of log files ({len(log_files)}) is not equal to 36.\")\n",
    "else:\n",
    "    print(\"All log files accounted for.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a little tricky here  \n",
    "\n",
    "Read in each of the logfiles, for each file extract:  \n",
    "- minimum temperature  \n",
    "- maximum temperature  \n",
    "- date of minimum temp   \n",
    "- date of maximum temp   \n",
    "- mean temp for each file.   \n",
    "\n",
    "This data should all be appended for a dictionary within a for loop:    \n",
    "Key should be the file name without the path or .txt extension  \n",
    "Values should be (minTemp,maxTemp,minDate,maxDate,meanTemp)\n",
    "\n",
    "I recommend making this work for one file first, then putting the rest in a for loop to do the rest.  \n",
    "\n",
    "Below is an example of how to read in one file\n",
    "\n",
    "*hint:* do not read date in as date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf8 in position 26: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2348\\3707873684.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Read in the log file as a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Readings (F)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf8 in position 26: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory path to the folder containing the log files\n",
    "directory = r'C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas'\n",
    "\n",
    "# Use glob to get all the log file paths in the directory\n",
    "log_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "\n",
    "# Initialize an empty dictionary to store the temperature data for each file\n",
    "temp_data = {}\n",
    "\n",
    "# Loop over the log files\n",
    "for log_file in log_files:\n",
    "    # Extract the file name without the path or extension\n",
    "    file_name = os.path.splitext(os.path.basename(log_file))[0]\n",
    "      \n",
    "    # Read in the log file as a DataFrame\n",
    "    df = pd.read_csv(log_file, delim_whitespace=True, header=None, names=['Date', 'Time', 'Readings (F)'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901302108_H7_D_14.txt.txt\n",
      "1901302109_H7_S_14.txt.txt\n",
      "1901302110_H13_S_14.txt.txt\n",
      "1901302115_H1_S_14.txt.txt\n",
      "1901302117_H15_S_14.txt.txt\n",
      "1901302118_H1_D_14.txt.txt\n",
      "1901302119_H4_S_14.txt.txt\n",
      "1901302120_H5_D_14.txt.txt\n",
      "1901302121_H8_S_14.txt.txt\n",
      "1901302122_H3_D_14.txt.txt\n",
      "1901302134_H13_D_14.txt.txt\n",
      "1901302136_H15_D_14.txt.txt\n",
      "1901302137_H18_S_14.txt.txt\n",
      "1901302138_H11_D_14.txt.txt\n",
      "1901302141_H19_S_14.txt.txt\n",
      "1901302146_H14_D_14.txt.txt\n",
      "1901302150_H14_S_14.txt.txt\n",
      "1901302157_H12_S_14.txt.txt\n",
      "1901302158_H9_S_14.txt.txt\n",
      "1901302194_H10_S_14.txt.txt\n",
      "1901302202_H16_S_14.txt.txt\n",
      "1901302203_H7_S_14.txt.txt\n",
      "1901302212_H9_D_14.txt.txt\n",
      "1901302214_H19_D_14.txt.txt\n",
      "1901302217_H18_D_14.txt.txt\n",
      "1901302223_H16_S_14.txt.txt\n",
      "1901302224_H3_S_14.txt.txt\n",
      "1901302225_H12_D_14.txt.txt\n",
      "1901302227_H11_S_14.txt.txt\n",
      "1901302228_H4_D_14.txt.txt\n",
      "1901302235_H6_S_14.txt.txt\n",
      "1901302236_H17_D_14.txt.txt\n",
      "1901302237_H10_D_14.txt.txt\n",
      "1901302240_H8_D_14.txt.txt\n",
      "1901302241_H2_D_14.txt.txt\n",
      "1901302243_H5_S_14.txt.txt\n"
     ]
    }
   ],
   "source": [
    "### set up data frame as you read in each file\n",
    "\n",
    "directory = r'C:\\Users\\Administrator\\Desktop\\BIOL_792\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas'\n",
    "\n",
    "# Use glob to get all the log file paths in the directory\n",
    "log_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "\n",
    "for log_file in log_files:\n",
    "    print(os.path.basename(log_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything in steps, make sure it works. Calculate summaries with this one infile:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, I suggest writing some dummy code in plain words to help outline your for loop:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles_dict = {}  \n",
    "for f in logfiles:  \n",
    "    #do something   \n",
    "    #do not read date in as date object  \n",
    "    #do more something   \n",
    "    #do other stuff  \n",
    "    #make print statements EVERYWHERE  \n",
    "    #append to dict  \n",
    "    #blahbahblah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the real code below here. You don't need to turnin your thoughts. Just put it in there as a help reminder. Most people all still do this, no matter how advanced they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have created a DataFrame with all the logfiles, print the head and save it to an outfile using pd.to_csv() as logfiles_df.csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is an example of the final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minTemp</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minDate</th>\n",
       "      <th>maxDate</th>\n",
       "      <th>meanTemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901302235_H6_S_14</th>\n",
       "      <td>21.3</td>\n",
       "      <td>65.8</td>\n",
       "      <td>12/6/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>35.726191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302121_H8_S_14</th>\n",
       "      <td>17.9</td>\n",
       "      <td>73.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.698866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302217_H18_D_14</th>\n",
       "      <td>21.0</td>\n",
       "      <td>82.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/26/2014</td>\n",
       "      <td>33.334564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302110_H13_S_14</th>\n",
       "      <td>22.8</td>\n",
       "      <td>60.9</td>\n",
       "      <td>10/4/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>32.240032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302146_H14_D_14</th>\n",
       "      <td>24.6</td>\n",
       "      <td>66.1</td>\n",
       "      <td>1/21/2014</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.242075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     minTemp  maxTemp     minDate    maxDate   meanTemp\n",
       "1901302235_H6_S_14      21.3     65.8   12/6/2013  7/13/2014  35.726191\n",
       "1901302121_H8_S_14      17.9     73.4  10/28/2013  7/13/2014  34.698866\n",
       "1901302217_H18_D_14     21.0     82.4  10/28/2013  7/26/2014  33.334564\n",
       "1901302110_H13_S_14     22.8     60.9   10/4/2013  7/13/2014  32.240032\n",
       "1901302146_H14_D_14     24.6     66.1   1/21/2014  7/13/2014  34.242075"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfiles_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
